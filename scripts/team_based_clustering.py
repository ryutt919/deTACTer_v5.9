# =========================================================
# team_based_clustering.py
# v5.6 Transformer ì„ë² ë”© ê¸°ë°˜ íŒ€ë³„ ë…ë¦½ í´ëŸ¬ìŠ¤í„°ë§
# =========================================================
# ì£¼ìš” ê¸°ëŠ¥:
# 1. Transformer ì„ë² ë”©ì„ íŒ€ë³„ë¡œ ë¶„í• 
# 2. ê° íŒ€ì— ëŒ€í•´ ë…ë¦½ì ìœ¼ë¡œ OPTICS í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
# 3. íŒ€ë³„ í´ëŸ¬ìŠ¤í„° ê²°ê³¼ë¥¼ ê°œë³„ íŒŒì¼ë¡œ ì €ì¥
# =========================================================

import pandas as pd
import numpy as np
import os
import sys
import yaml
from sklearn.cluster import OPTICS

# í•œê¸€ ì¶œë ¥ ì„¤ì •
if sys.stdout.encoding != 'utf-8':
    try:
        sys.stdout.reconfigure(encoding='utf-8')
    except:
        pass

# =========================================================
# ì„¤ì • ë¡œë“œ
# =========================================================
# ì„¤ì • ë¡œë“œ
# v5.9: ê²½ë¡œ ì†Œí”„íŠ¸ ì½”ë”© ë° ë²„ì „ ê´€ë¦¬ ì •í•©ì„± ê°•í™”
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
CONFIG_PATH = os.path.join(BASE_DIR, 'config.yaml')

with open(CONFIG_PATH, 'r', encoding='utf-8') as f:
    config = yaml.safe_load(f)

VERSION = config['version']
REFINED_DIR = os.path.join(BASE_DIR, 'data', 'refined', VERSION)
EMBEDDINGS_DIR = os.path.join(BASE_DIR, 'results', 'embeddings', VERSION)
OUTPUT_DIR = os.path.join(BASE_DIR, 'results', 'clustering', VERSION, 'transformer')

os.makedirs(OUTPUT_DIR, exist_ok=True)

# =========================================================
# íŒ€ë³„ í´ëŸ¬ìŠ¤í„°ë§ í•¨ìˆ˜
# =========================================================
def perform_team_clustering(version='v5.6'):
    """
    v5.6 Transformer ì„ë² ë”©ì„ íŒ€ë³„ë¡œ ë¶„í• í•˜ì—¬ í´ëŸ¬ìŠ¤í„°ë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    print(f"=" * 60)
    print(f"íŒ€ë³„ Transformer í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘ (ë²„ì „: {version})")
    print(f"=" * 60)
    
    # 1. ì„ë² ë”© ë° ë©”íƒ€ë°ì´í„° ë¡œë“œ
    embeddings_path = os.path.join(EMBEDDINGS_DIR, 'transformer_embeddings.npy')
    metadata_path = os.path.join(EMBEDDINGS_DIR, 'transformer_metadata.csv')
    sequences_path = os.path.join(REFINED_DIR, 'attack_sequences.csv')
    
    if not os.path.exists(embeddings_path):
        print(f"[ì˜¤ë¥˜] ì„ë² ë”© íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {embeddings_path}")
        return
    
    print(f"\n[1/4] ë°ì´í„° ë¡œë“œ ì¤‘...")
    embeddings = np.load(embeddings_path)
    metadata_df = pd.read_csv(metadata_path, encoding='utf-8-sig')
    sequences_df = pd.read_csv(sequences_path, encoding='utf-8-sig')
    
    # [Fix] ë©”íƒ€ë°ì´í„° ì¤‘ë³µ ì œê±° (ì •í•©ì„± ê°•í™”)
    initial_meta_count = len(metadata_df)
    metadata_df = metadata_df.drop_duplicates(subset=['sequence_id']).reset_index(drop=True)
    if len(metadata_df) < initial_meta_count:
        print(f"  âš ï¸ ë©”íƒ€ë°ì´í„° ì¤‘ë³µ ë°œê²¬: {initial_meta_count} -> {len(metadata_df)}")
    
    # ë©”íƒ€ë°ì´í„°ì— ì›ë³¸ ì„ë² ë”© ì¸ë±ìŠ¤ ì¶”ê°€ (ì¡°ì¸ í›„ì— ì˜¬ë°”ë¥¸ ì„ë² ë”©ì„ ê°€ì ¸ì˜¤ê¸° ìœ„í•´ í•„ìˆ˜)
    metadata_df['embedding_idx'] = range(len(metadata_df))
    
    # 2. ë©”íƒ€ë°ì´í„°ì™€ ì‹œí€€ìŠ¤ ë°ì´í„° ì¡°ì¸ (íŒ€ëª… ì •ë³´ íšë“)
    print(f"[2/4] íŒ€ ì •ë³´ ë§¤ì¹­ ì¤‘...")
    # [Fix] ì‹œí€€ìŠ¤ ë°ì´í„°ì—ì„œ IDë³„ íŒ€ëª… ê³ ìœ ì„± í™•ë³´
    sequences_unique = sequences_df.groupby('sequence_id').first().reset_index()
    
    merged_df = metadata_df.merge(
        sequences_unique[['sequence_id', 'team_name_ko']],
        on='sequence_id',
        how='left'
    )
    
    # [Fix] ìµœì¢… ì¡°ì¸ í›„ì—ë„ ì¤‘ë³µ ë°œìƒ ì—¬ë¶€ ì¬í™•ì¸ (ë°©ì–´ì  ì½”ë”©)
    merged_df = merged_df.drop_duplicates(subset=['sequence_id'])
    
    # íŒ€ëª…ì´ ì—†ëŠ” ê²½ìš° ì œì™¸
    merged_df = merged_df.dropna(subset=['team_name_ko'])
    
    # 3. íŒ€ë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
    teams = merged_df['team_name_ko'].unique()
    print(f"[3/4] ì´ {len(teams)}ê°œ íŒ€ ë°œê²¬")
    
    for team_name in teams:
        print(f"\nâ–¶ï¸ [{team_name}] í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘...")
        
        # í•´ë‹¹ íŒ€ì˜ ë°ì´í„°ë§Œ í•„í„°ë§
        team_data = merged_df[merged_df['team_name_ko'] == team_name].copy()
        
        # embedding_idxë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜¬ë°”ë¥¸ ì„ë² ë”© ì¶”ì¶œ
        emb_indices = team_data['embedding_idx'].tolist()
        team_embeddings = embeddings[emb_indices]
        
        if len(team_embeddings) < 5:
            print(f"  âš ï¸ ë°ì´í„° ë¶€ì¡± (ìƒ˜í”Œ ìˆ˜: {len(team_embeddings)}), ìŠ¤í‚µ")
            continue
        
        # OPTICS í´ëŸ¬ìŠ¤í„°ë§
        # íŒ€ë³„ ë°ì´í„°ê°€ ì ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ min_samplesëŠ” ë°ì´í„° í¬ê¸°ì— ë§ì¶° ì¡°ì •í•˜ë˜ config ê°’ì„ ìƒí•œìœ¼ë¡œ ì‚¬ìš©
        current_min_samples = min(config['clustering'].get('min_samples', 3), len(team_embeddings) // 2)
        current_min_samples = max(2, current_min_samples) # ìµœì†Œ 2ê°œëŠ” ë˜ì–´ì•¼ í•¨
        
        optics = OPTICS(min_samples=current_min_samples, xi=config['clustering'].get('xi', 0.01), metric='euclidean')
        cluster_labels = optics.fit_predict(team_embeddings)
        
        # í´ëŸ¬ìŠ¤í„° ë ˆì´ë¸” ì¶”ê°€
        team_data['cluster'] = cluster_labels
        
        # í´ëŸ¬ìŠ¤í„° í†µê³„
        n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)
        n_noise = list(cluster_labels).count(-1)
        print(f"  âœ… í´ëŸ¬ìŠ¤í„° ìˆ˜: {n_clusters}, ë…¸ì´ì¦ˆ: {n_noise}")
        
        # íŒ€ë³„ ë””ë ‰í† ë¦¬ ìƒì„± ë° ì €ì¥
        team_output_dir = os.path.join(OUTPUT_DIR, team_name)
        os.makedirs(team_output_dir, exist_ok=True)
        
        output_path = os.path.join(team_output_dir, 'team_clusters.csv')
        team_data[['sequence_id', 'cluster']].to_csv(output_path, index=False, encoding='utf-8-sig')
        print(f"  ğŸ’¾ ì €ì¥ ì™„ë£Œ: {output_path}")
    
    print(f"\n{'='*60}")
    print(f"ëª¨ë“  íŒ€ë³„ í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ!")
    print(f"{'='*60}")

# =========================================================
# ë©”ì¸ ì‹¤í–‰
# =========================================================
if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="íŒ€ë³„ Transformer í´ëŸ¬ìŠ¤í„°ë§")
    parser.add_argument("--version", type=str, default="v5.6", help="ë°ì´í„° ë²„ì „")
    args = parser.parse_args()
    
    perform_team_clustering(version=args.version)
